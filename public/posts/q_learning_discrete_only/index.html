<!doctype html>
<html
    lang="en-us"
    dir="ltr"
>
    <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
        <meta charset="utf-8" />
<meta name="viewport" content="width=device-width" />
<title>
    Why doesn&#39;t Q-learning work with continuous actions? | Decisions &amp; Dragons
</title>
<link
    href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap"
    rel="stylesheet"
/>

    <link rel="stylesheet" href="/css/main.css">
    <link rel="stylesheet" href="/css/fonts.css">
    <link rel="stylesheet" href="/css/home.css">
    <link rel="stylesheet" href="/css/page_without_toc.css">
    <link rel="stylesheet" href="/css/page_with_toc.css">
    <link rel="stylesheet" href="/css/content_structure.css">
    <link rel="stylesheet" href="/css/mdx.css">
 
      <script src="/js/main.js"></script>

<meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']] , 
      tags: 'ams'
    }
  };
</script>  <script>
window.addEventListener("hashchange", function () {
    window.scrollTo(window.scrollX, window.scrollY - 70);
});
</script>
    </head>
    <body>
        
<div class="small_header">
    <span
        ><a class="logo_link" href="http://localhost:1313/"
            >Decisions &amp; Dragons</a
        ></span
    >
    <span class="nav">
        
        <a href=" /">Home</a>
        
        <a href=" /about/">About</a>
        
        <a href=" /posts/">All posts</a>
        
        <a href=" /notation/">Notaiton</a>
        
        <a href=" https://twitter.com/jmac_ai">Twitter</a>
        
    </span>
</div>
<div class="toc-content-container">
    <div class="sidebar">
        <span style="text-align: center"><h3>Table of Contents</h3></span>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#examining-the-q-learning-update">Examining the Q-learning update</a></li>
    <li><a href="#extracting-the-policy-is-no-easier">Extracting the policy is no easier</a></li>
    <li><a href="#alternatives">Alternatives</a></li>
  </ul>
</nav>
    </div>
    <div class="rhs-main-content">
        <h1>Why doesn&#39;t Q-learning work with continuous actions?</h1>
        <p>Q-learning requires finding the action with the maximum Q-value in two places: (1) In the learning update itself; and (2) when extracting the policy from the learned Q-values. When there are a small number of discrete actions, you can simply enumerate the Q-values for each and pick the action with the highest value. However, this approach does not work with continuous actions, because there are an infinite number of actions to evaluate!</p>
<h2 id="examining-the-q-learning-update">Examining the Q-learning update</h2>
<p>Lets take a look at the core Q-learning update rule:</p>
<p>$$
Q(s, a) \gets Q(s, a) \alpha \left[ r + \gamma \max_{a&rsquo;} Q(s&rsquo;, a&rsquo;) - Q(s, a) \right],
$$</p>
<p>where $r$ is the observed reward and $s&rsquo;$ is the next observed state.</p>
<p>Or if we’re working with function approximation of the Q function (e.g., using a neural net), we can frame this as a loss minimization problem where the loss function to minimize is:</p>
<p>$$
L(\theta) \triangleq \frac{1}{2}\left(r + \gamma \max_{a&rsquo;} Q_{\theta^-}(s&rsquo;, a&rsquo;) - Q_\theta(s, a) \right)^2,
$$</p>
<p>where $\theta^-$ is a copy of the parameters $\theta$ that we do not differentiate through (and in the case of DQN, may be many updates old).</p>
<p>Effectively we’re doing a standard supervised regression problem on our function approximation of Q, except instead of having pre-defined labels in a dataset, we compute a “label” on each step with the value:</p>
<p>$$
y \triangleq r + \gamma \max_{a&rsquo;} Q_{\theta^-}(s&rsquo;, a&rsquo;).
$$</p>
<p>You might think that as long as our Q-function network operats on continuous actions we should be alright. After all, neural networks train on all kind of continuous inputs, why not the action?</p>
<p>Except how are you going to compute that label? Do you see the problem? It’s that damn $\max$ operator!</p>
<p>If your actions are continuous, and therefore infinite, you can’t just look up which action produces the maximum q-value because there are an infinite number of actions to scan.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<h2 id="extracting-the-policy-is-no-easier">Extracting the policy is no easier</h2>
<p>This same problem emerges when we want to extract the policy from the Q-values, because the greedy policy from the learned Q-values (which is optimal when the optimal Q-values are leared) is:</p>
<p>$$
\pi(s) = \arg\max_a Q(s, a).
$$</p>
<p>So when we want to act in the world, here too does the curse of the max operator thwart us. Even if you use epsilon-greedy, the greedy part of the policy is going to requrie you to find the action with the maximum Q-value!</p>
<h2 id="alternatives">Alternatives</h2>
<p>Taking a step back, we <em>mostly</em> already know how to solve this problem in AI: finding the the max is just an optimization problem! Therefore, you can just replace the max operator with your favorite optimization algorithm. For example <a href="https://arxiv.org/abs/1806.10293">QT-OPT</a>, uses the cross entropy method to approximate the max. Or you could use SGD on the Q-function to find the maximum action.</p>
<p>However, if you’re going to replace the exact max with the result of some optimization algorithm you’re</p>
<ol>
<li>probably increasing the compute time for both training (you’re doing optimization in the inner loop of an optimization algorithm!) and inference/acting; and</li>
<li>only going to end up with an approximate max anyway which, depending on details, may get stuck in local optima.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></li>
</ol>
<p>That’s not to say those approaches can’t be successful, but it’s a fairly significant deviation from the classic Q-learning algorithm that will affect the solution conept the algorithm finds. I&rsquo;ll let you decided whether to still call algorithms that use these modifications Q-learning.</p>
<p>Personally, I prefer actor-critic methods for continuous actions since they do not suffer this same problem. And if you want an actor-critic method that is quite similar to Q-learning, you may want to consider <a href="https://arxiv.org/abs/1509.02971">DDPG</a>, <a href="https://arxiv.org/abs/1802.09477v3">TD3</a>, or <a href="https://arxiv.org/abs/1801.01290">SAC</a>. If you want to learn more about how DDPG (and largely TD3) works, you may want to see <a href="../ddpg_grad/">my post</a> on its policy gradient derivation.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Okay, <em>nothing</em> is actually continuous in computers. Computer numbers are actually discrete and we just use clever discrete encodings like float32 to represent a large (but finite!) set of fractional values. But there&rsquo;s still a hell of lot of possible values that you wouldn&rsquo;t want to enumerate! Like, over 4 billion. And that&rsquo;s if you only have <em>one</em> continuous action.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Unless your Q-function is convex on the actions or has some other really convenient, but restrictive, properties.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
    </div>
</div>

    </body>
</html>
