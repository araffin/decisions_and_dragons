<!doctype html>
<html
    lang="en-us"
    dir="ltr"
>
    <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
        <meta charset="utf-8" />
<meta name="viewport" content="width=device-width" />
<title>
    Why does experience replay require off-policy learning and how is it different from on-policy learning? | Decisions &amp; Dragons
</title>
<link
    href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap"
    rel="stylesheet"
/>

<link rel="stylesheet" href="/css/main.css" />
<link rel="stylesheet" href="/css/fonts.css" />
<link rel="stylesheet" href="/css/mdx.css" />
 
      <script src="/js/main.js"></script>

<meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']] , 
      tags: 'ams'
    }
  };
</script>  <script>
window.addEventListener("hashchange", function () {
    window.scrollTo(window.scrollX, window.scrollY - 70);
});
</script>
    </head>
    <body>
        
<div class="small_header">
    <span
        ><a class="logo_link" href="http://localhost:1313/"
            >Decisions &amp; Dragons</a
        ></span
    >
    <span class="nav">
        
        <a href=" /about/">About</a>
        
        <a href=" /notation/">Notation</a>
        
        <a href=" /posts/">Posts</a>
        
    </span>
</div>
<div class="toc-content-container">
    <div class="sidebar">
        <div class="toc_container">
            <span style="text-align: center"><h3>Table of Contents</h3></span>
            <nav id="TableOfContents">
  <ul>
    <li><a href="#off-policy-vs-on-policy">Off-policy vs on-policy</a></li>
    <li><a href="#evaluation-vs-improvement-and-the-strange-case-of-ppo">Evaluation vs improvement and the strange case of PPO</a>
      <ul>
        <li><a href="#evaluation-vs-improvement">Evaluation vs improvement</a></li>
        <li><a href="#ppo">PPO</a></li>
      </ul>
    </li>
    <li><a href="#is-on-policy-or-off-policy-better">Is on-policy or off-policy better?</a></li>
  </ul>
</nav>
        </div>
    </div>
    <div class="main-content">
        <h1>Why does experience replay require off-policy learning and how is it different from on-policy learning?</h1>
        <p>When you use an experience replay buffer, you save the most recent $k$ experiences of the agent, and sample data from that buffer for training. Typically, the agent does a step of training to update its policy for every step in the environment. At any moment in time, the vast majority of experiences in the buffer are generated with a different &ndash; earlier &ndash; policy than the current policy. And if the policy used to collect data is different than the policy being evaluated or improved, then you need an off-policy method.</p>
<h2 id="off-policy-vs-on-policy">Off-policy vs on-policy</h2>
<p>There is often confusion about the meaning of off-policy and on-policy. Many people use &ldquo;on-policy&rdquo; to refer to any method that evaluates an explicit policy. In this definition, any actor-critic method would be an &ldquo;on-policy&rdquo; method because in actor-critic methods, the actor is an explicit parameterized policy that the critic (the value function) evaluates. While many actor-critic methods are on-policy, the term classically means something different, and you can in fact have off-policy actor-critic methods.</p>
<p>The classic meaning of on-policy vs off-policy regards whether your training method requires your training data to be collected from the policy to be evaluated and improved, or whether it can be used even if your data is collected from a different policy.</p>
<p>If you hold your policy constant and then collect a buch of data with it, then this data distribution is on-policy and you can use an on-policy method to evaluate/improve it.</p>
<p>If your data is generated by some other policy, be it an exploration policy, older versions of your policy, or maybe even some other expert, then you will need an off-policy method. Since the experience replay buffer is dominated by data generated by earlier versions of the agent&rsquo;s policy, you will need an off-policy method to do policy evaluation/improvement from it.</p>
<h2 id="evaluation-vs-improvement-and-the-strange-case-of-ppo">Evaluation vs improvement and the strange case of PPO</h2>
<p>You may have noticed I keep naming two cases where on/off policy is relevant: for policy evaluation and policy improvement. For most algorithms, both the evaluation and improvement will be on-policy or off-policy. However, evaluation and improvement are two distinct steps. You could have one part be on-policy while the other is off-policy. PPO is an example where the policy evaluation is on-policy while the improvement is off-policy.</p>
<h3 id="evaluation-vs-improvement">Evaluation vs improvement</h3>
<p>First, let&rsquo;s give some definitions to policy evaluation/improvement. These terms come from the steps of policy iteration, the foundation for many RL methods. In policy iteration, you repeat two steps until a policy stops improving.</p>
<ol>
<li>Evaluate $Q^\pi(s, a)$ for your current policy $\pi$ for all state-action pairs.</li>
<li>Improve your policy $\pi$ by updating it to maximize $Q(s, \pi_\theta(s))$ for each state.</li>
</ol>
<p>In the first evaluation step, we evaluate the Q-function for the given policy. It is worth noting that we don&rsquo;t have to <em>explicitly</em> model $Q^\pi$. There are other approaches we could take such as explicitly modeling the state value function $V^\pi(s)$, and then <em>implicitly</em> derive $Q^\pi$ with observed transitions. Alternatively, we could explicitly model $V^\pi$ and the environment transition dynamics $T(s&rsquo; | s, a)$ from which we could derive $Q^\pi$.</p>
<p>Regardless of whether we explicitly or implicitly model $Q^\pi$, &ldquo;evaluation&rdquo; refers to estimating a value function for a policy. If you are having difficulty understanding the exaction definition of value functions and difference between $Q$ and $V$, you may want look at my answer to <a href="../q_vs_v">this question</a>.</p>
<p>The term &ldquo;improvement&rdquo; regards the second step: how you make your policy better maximize the value function. As you might expect, there are many different ways to improve your policy given a value function estimate.</p>
<p>Because evaluation and improvement are separate steps, you can use different methods and data to perform them. Let&rsquo;s briefly review the core idea behind PPO to help explain how you might perform these steps differently.</p>
<h3 id="ppo">PPO</h3>
<p>PPO is roughly the following algorithm.</p>
<pre tabindex="0"><code>Initialize state value function V.
Initialize policy pi.
Do forever:
  Collect k n-length trajecories
  For each trajectory i to k:
    For each step j to n:
      // compute returns
      Rij = discounted sum of rewards j to n
      // compute advantages
      Aij = Rij - V(sij)
  For M SGD steps:
    Update V(sij) toward Rij
    Update policy pi using PPO CLIP
</code></pre><p>Here, the PPO CLIP objective is defined as</p>
<p>$$
L(s, a, \theta_\text{old}, \theta) = \min\left(\frac{\pi_\theta(a | s)}{\pi_{\theta_\text{old}}(a | s)}A(s, a), \text{clip}\left(\frac{\pi_\theta(a | s)}{\pi_{\theta_\text{old}}(a | s)}, 1-\epsilon, 1+\epsilon \right)A(s, a) \right),
$$</p>
<p>where $\pi_{\theta_\text{old}}$ is the policy we used to the collect the $k$ trajectories before doing any updates.</p>
<p>PPO is an interesting case because its evaluation method is on-policy, while its policy improvement is off-policy. That is, if you look at the above algorithm, we are updating V (over multiple steps of SGD) toward value targets of the <em>behavior</em> policy we used to collect the data. Although we are simultaneously improving the policy, the value function is not evaluating the new updated policy, it only evaluates the behavior policy.</p>
<p>At the same time the behavior policy is evaluated, the policy improvement is performed multiple times through multiple steps of SGD. On the first SGD improvement step, the behavior policy and current policy match, resulting in an on-policy method update. However, after that first step in which the policy is updated, we now have a different policy than the behavior policy, resulting in the data being slightly off-policy. PPO&rsquo;s policy update accounts for the off-policy data in two ways. First, it uses an importance sampling ratio to correct for the difference in distributions. That&rsquo;s the $\frac{\pi_\theta(a | s)}{\pi_{\theta_\text{old}}(a | s)}$ term. Second, it clips the updates once the policy drifts too far from the behavior policy, ensuring the data it has is close enough to provide good estimates of the true policy objective. If you don&rsquo;t understand how the importance sampling ratio corrects for off-policy data, <a href="../q_learning_doesnt_need_importance_sampling/#importance-sampling">see my discussion about it here</a>.</p>
<p>So, although many RL methods are either off-policy or on-policy for both evaluaiton and improvement, this need not be the case. PPO is an example where the evaluation is on-policy (it evaluates the behavior policy), while the improvement step is off-policy (it improves a policy that is different than the behavior policy).</p>
<h2 id="is-on-policy-or-off-policy-better">Is on-policy or off-policy better?</h2>
<p>There is no clear answer to whether on-policy of off-policy is better. Off-policy is a more preferable setting, because it means we can learn from a wider source of data, while on-policy methods are more wasteful and requires us to get more data every time we change our policy. However, at this current moment in time, on-policy methods tend to be more stable than off-policy methods. So if gathering data from your policy is cheap, you might prefer to use an on-policy method.</p>
    </div>
</div>

    </body>
</html>
