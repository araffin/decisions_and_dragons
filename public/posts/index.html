<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Posts | Decisions &amp; Dragons</title>
<link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="/css/main.css">


      <script src="/js/main.js"></script>


  
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']] , 
      tags: 'ams'
    }
  };
</script>
  
  <script>
window.addEventListener("hashchange", function () {
    window.scrollTo(window.scrollX, window.scrollY - 70);
});
</script>
</head>
<body>
    
<div id="small_splash">
    <span><a class="logo_link" href="/">Decisions &amp; Dragons</a></span>
    <span class="nav">
         
            <a href=" /">Home</a>
         
            <a href=" /about/">About</a>
         
            <a href=" /posts/">All posts</a>
         
            <a href=" https://twitter.com/jmac_ai">Twitter</a>
        
    </span>
  </div>

    
  <div id="content_no_toc">
    <h1>Posts</h1>
    
    
      <h2><a class="post_link" href="/posts/q_vs_v/">What is the difference between V(s) and Q(s,a)?</a></h2>
      State value function $V(s)$ expresses how well the agent expects to do when it acts normally. $Q(s, a)$ is a counterfactual function that expresses how well the agent expects to do if first takes some potentially alternative action before acting normally.
A more precise definition Before making that more precise, let&rsquo;s first add a point of clarity. Value functions (either V or Q) are always with respect to some policy $\pi$. <a href="/posts/q_vs_v/">[See more]</a>
    
      <h2><a class="post_link" href="/posts/why_does_the_policy_gradient_include_log_prob/">Why does the policy gradient include a log probobability term?</a></h2>
      Actually, it doesn&rsquo;t! What you&rsquo;re probably thinking of is the REINFORCE esitmate of the policy gradient that on-policy methods often use in practice. How we derive the REINFORCE estimate you&rsquo;re familiar with and why we use it is something I found to be glossed over in literature, or presented in an overly technical way, but fortunately, it is not a hard concept to learn!
The short answer for where the log probability comes from is that REINFORCE uses a trick to make the policy gradient look like an expected value over the gradient, and the log probability manfiests as an importance sampling correction to make this work. <a href="/posts/why_does_the_policy_gradient_include_log_prob/">[See more]</a>
    
      <h2><a class="post_link" href="/posts/model_free_vs_model_based/">What is the difference between model-based and model-free RL?</a></h2>
      In reinforcement learning, the agent is not assumed to know how the environment will be affected by its actions. Model-based and model-free reinforcement learning tackle this problem in different ways. In model-based reinforcement learning, the agent learns a model of how the environment is affected by its actions and uses this model to determine good behavior. In model-free reinforcement learning, the agent learns how to act well without ever learning to predict precisely how the environment will be affectd by its actions. <a href="/posts/model_free_vs_model_based/">[See more]</a>
    
  </div>

  <footer>
    <div style="text-align: center;">

</div>
  </footer>
</body>
</html>
