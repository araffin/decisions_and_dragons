<!doctype html>
<html
    lang="en-us"
    dir="ltr"
>
    <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
        <meta charset="utf-8" />
<meta name="viewport" content="width=device-width" />
<title>
    What is the difference between model-based and model-free RL? | Decisions &amp; Dragons
</title>
<link
    href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap"
    rel="stylesheet"
/>

<link rel="stylesheet" href="/css/main.css" />
<link rel="stylesheet" href="/css/fonts.css" />
<link rel="stylesheet" href="/css/mdx.css" />
 
      <script src="/js/main.js"></script>

<meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']] , 
      tags: 'ams'
    }
  };
</script>  <script>
window.addEventListener("hashchange", function () {
    window.scrollTo(window.scrollX, window.scrollY - 70);
});
</script>
    </head>
    <body>
        
<div class="small_header">
    <span
        ><a class="logo_link" href="http://localhost:1313/"
            >Decisions &amp; Dragons</a
        ></span
    >
    <span class="nav">
        
        <a href=" /about/">About</a>
        
        <a href=" /notation/">Notation</a>
        
        <a href=" /posts/">Posts</a>
        
    </span>
</div>
<div class="toc-content-container">
    <div class="sidebar">
        <div class="toc_container">
            <span style="text-align: center"><h3>Table of Contents</h3></span>
            <nav id="TableOfContents">
  <ul>
    <li><a href="#mdps">MDPs</a></li>
    <li><a href="#planning-vs-rl">Planning vs RL</a></li>
    <li><a href="#model-based-rl">Model-based RL</a></li>
    <li><a href="#model-free-rl">Model-free RL</a></li>
    <li><a href="#is-one-approach-better-than-another">Is one approach better than another?</a></li>
    <li><a href="#a-simple-heuristic">A simple heuristic</a></li>
  </ul>
</nav>
        </div>
    </div>
    <div class="main-content">
        <h1>What is the difference between model-based and model-free RL?</h1>
        <p>In reinforcement learning, the agent is not assumed to know how the environment will be affected by its actions. Model-based and model-free reinforcement learning tackle this problem in different ways. In model-based reinforcement learning, the agent learns a model of how the environment is affected by its actions and uses this model to determine how to act. In model-free reinforcement learning, the agent learns how to act without ever learning to predict precisely how the environment will be affected by its actions.</p>
<p>To better understand this distinction more completely, it is helpful to revisit the definition of an MDP, how to solve it, and how RL makes this problem harder.</p>
<h2 id="mdps">MDPs</h2>
<p>Let&rsquo;s revisit the components of an MDP, the most typical decision-making framework for RL. An MDP is defined by a 4-tuple $(S, A, R, T)$ where,</p>
<ul>
<li>$S$ is the state/observation space of an environment,</li>
<li>$A$ is the set of actions the agent can choose between,</li>
<li>R(s, a) is a function that returns the reward received for taking action $a$ from state $s$,</li>
<li>$T(s&rsquo; | s, a)$ is a transition probability function, specifying the probability that the environment will transition to state $s&rsquo;$ if the agent takes action $a$ in state $s$.</li>
</ul>
<p>Our goal is to find a policy $\pi$, mapping states to action choices we should make in each state, that maximizes the expected future (discounted) reward.</p>
<h2 id="planning-vs-rl">Planning vs RL</h2>
<p>If we know what all those elements of an MDP are, we can compute a good policy before ever actually executing an action in the environment. In AI, we typically call computing the solution to a decision-making problem before executing an actual decision &ldquo;planning.&rdquo; Some classic planning algorithms for MDPs include Value Iteration, Policy Iteration, Monte Carlo Tree Search, and whole lot more.</p>
<p>But the RL problem isn’t so kind to us. What makes a problem an RL problem, rather than a planning problem, is the agent does <em>not</em> know all the elements of the MDP, precluding it from being able to plan a solution.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> Specifically, the agent does not know how the environment will change in response to its actions (the transition function $T$), nor what immediate reward it will receive for doing so (the reward function $R$). The agent will simply have to try actions in the environment, observe what happens, and somehow, find a good policy from doing so.</p>
<p>How can an agent find a good policy if it does not know the transition function $T$ nor the reward function $R$? It turns out there are lots of ways!</p>
<h2 id="model-based-rl">Model-based RL</h2>
<p>One approach that might immediately strike you is for the agent to learn a model of how the environment works and then plan a solution using that model. That is, suppose the agent is currently in state $s_1$, takes action $a_1$ and then observes that the environment transitions to state $s_2$ with reward $r_2$. That obserevation can be used as training data to predict $T(s_2 | s_1, a_1)$ and $R(s_1, a_1) \rightarrow r_2$ with supervised learning.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> Using its learned models of $T$ and $R$, the agent can plan a solution. And as the the models get better, it can form more accurate plans. RL solutions that follow this framework are model-based RL algorithms.</p>
<h2 id="model-free-rl">Model-free RL</h2>
<p>As it turns out though, we don’t have to learn a model of the environment to find a good policy. One of the most classic examples is Q-learning, which directly estimates the optimal Q-values of each action in each state.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> A policy may then be derived from the learned Q-values by choosing the action with the highest Q-value in the current state. Other model-free methods include actor-critic and policy search methods, which search over the policy space to find policies that result in better reward from the environment (sometimes while still learning Q-functions!).</p>
<p>Because these approaches do not learn a model of the environment they are called model-free algorithms. Model-free methods demonstrate that you do not need to know what the next state for any given action is to act well. You just need to be able to estimate which actions in which states will allow you to collect a lot of future reward.</p>
<h2 id="is-one-approach-better-than-another">Is one approach better than another?</h2>
<p>Answering which is better is rather complicated. Model-free methods may be less biased because they rely more closely on real experiences. It turns out it&rsquo;s hard to learn good models of the environment, and if your model used for planning is poor, you may make bad decisions. However, model-free methods tend to require a lot of interaction with the environment and have some tricky algorithm stability issues. For the time being, you will just have to figure out what works best for your environment.</p>
<h2 id="a-simple-heuristic">A simple heuristic</h2>
<p>If you want a way to check if an RL algorithm is model-based or model-free, ask yourself this question: after learning, can the agent make predictions about what the next state and reward will be before it takes each action? If it can, then it’s a model-based RL algorithm. if it cannot, it’s a model-free algorithm.</p>
<p>This same idea may also apply to decision-making processes other than MDPs.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Beware: I&rsquo;m entering into controversial territory here! There are methods that use knowledge of the transitions dynamics that some people will call &ldquo;RL.&rdquo; Like most things, lines get blurry. For example, the authors of AlphaGo called their method &ldquo;RL&rdquo; even though they used MCTS and the dynamics of the game were known and provided to the algorithm. Usually, when people call &ldquo;planning&rdquo; methods &ldquo;RL&rdquo; methods, they do so because the method makes limited use of its knowledge of the transition dynamics. Typically for computational reasons. Nevertheless, methods like MCTS require the agent to have knowledge of how the environment is affected by its actions, and they exploit this knowledge. For this reason, I still call them &ldquo;planning&rdquo; methods. Call me a curmudgeon if you must.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Tranining the transition function predict $T(s_2 | s_1, a_1)$ can be a little complicated, because $T(s_2 | s_1, a_1)$ is meant to represent the probability of reaching state $s_2$. If we can assume the environment is deterministic with small levels of noise, we can train a function to predict $s_2$ given $s_1$ and $a_1$, and assign its output probability 1. If the environment is more stochastic, things get a little harder. However, generative ML is increasingly getting better. While generative models often don&rsquo;t provide exact probabilities, there are many planning methods that only require a generative model and do not need a full probability distribution.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>A Q-value $Q(s, a)$ is an estimate of how much long-term reward the agent expects to get if it takes action $a$ from state $s$. If you don&rsquo;t feel comfortable with the definition of a Q-value, see <a href="../q_vs_v/">my answer to this question</a>.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
    </div>
</div>

    </body>
</html>
