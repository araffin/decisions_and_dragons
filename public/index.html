<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
	<meta name="generator" content="Hugo 0.124.1"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Decisions &amp; Dragons</title>
<link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="/css/main.css">


      <script src="/js/main.js"></script>


  
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']] , 
      tags: 'ams'
    }
  };
</script>
  
  <script>
window.addEventListener("hashchange", function () {
    window.scrollTo(window.scrollX, window.scrollY - 70);
});
</script>
</head>
<body>
    
<div id="splash">
<h1>Decisions &amp; Dragons</h1>
<p>A guide to the perilous world of reinforcement learning.</p>
</div>

    
  <div id="content_center">
    <div id="home_nav">
      <ul> 
         
            <li><a href=" /">Home</a></li>
         
            <li><a href=" /about/">About</a></li>
         
            <li><a href=" /posts/">All posts</a></li>
         
            <li><a href=" https://twitter.com/jmac_ai">Twitter</a></li>
        
      </ul>
    </div>
    <p>This site is a collection of answers to question about
reinforcement learning that I&rsquo;ve given over the years on various social media sites.
It also includes occasional opinion pieces of AI in general.</p>
<p>You can learn more about
me and why I created this site in the <a href="/about/">about section</a>. You can find
a list of all posts in the <a href="/posts/">posts section</a>.</p>

    <h2>Recent posts</h2>
    
      <h3><a class="post_link" href="/posts/q_learning_doesnt_need_importance_sampling/">If Q-learning is off-policy, why doesn&#39;t it require importance sampling?</a></h3>
      <p>In off-policy learning, we are evaluate the value function for a policy other than the one we are following in the environment. This difference creates a mismatch in state-action distributions. To account for this difference many actor-critic methods use importance sampling. However, Q-learning, perhaps the most famous off-policy method that evaluates the optimal policy, does not. There is a simple reason for that: In Q-learning, samples are only used to average over the transition dynamics, not the policy. Still confused? Let&rsquo;s make that more concrete with a simple example and derive different ways to solve it.</p> <a href="/posts/q_learning_doesnt_need_importance_sampling/">[See more]</a>
    
      <h3><a class="post_link" href="/posts/q_vs_v/">What is the difference between V(s) and Q(s,a)?</a></h3>
      <p>State value function $V(s)$ expresses how well the agent expects to do when it acts normally. $Q(s, a)$ is a counterfactual function that expresses how well the agent expects to do if first takes some potentially alternative action before acting normally.</p> <a href="/posts/q_vs_v/">[See more]</a>
    
      <h3><a class="post_link" href="/posts/why_does_the_policy_gradient_include_log_prob/">Why does the policy gradient include a log probobability term?</a></h3>
      <p>Actually, it doesn&rsquo;t! What you&rsquo;re probably thinking of is the <a href="https://people.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf">REINFORCE</a> <em>esitmate</em> of the policy gradient that on-policy methods often use in practice. How we derive the REINFORCE estimate you&rsquo;re familiar with and <em>why</em> we use it is something I found to be glossed over in literature, or presented in an overly technical way, but fortunately, it is not a hard concept to learn!</p> <a href="/posts/why_does_the_policy_gradient_include_log_prob/">[See more]</a>
    
  </div>

  <footer>
    <div style="text-align: center;">

</div>
  </footer>
</body>
</html>
