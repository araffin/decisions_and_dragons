<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
	<meta name="generator" content="Hugo 0.124.1"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Decisions &amp; Dragons</title>
<link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="stylesheet" href="/css/fonts.css">
    <link rel="stylesheet" href="/css/home.css">
    <link rel="stylesheet" href="/css/page_without_toc.css">
    <link rel="stylesheet" href="/css/page_with_toc.css">
    <link rel="stylesheet" href="/css/content_structure.css">
    <link rel="stylesheet" href="/css/mdx.css">


      <script src="/js/main.js"></script>


  
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']] , 
      tags: 'ams'
    }
  };
</script>
  
  <script>
window.addEventListener("hashchange", function () {
    window.scrollTo(window.scrollX, window.scrollY - 70);
});
</script>
</head>
<body>
    
<div id="splash">
<h1>Decisions &amp; Dragons</h1>
<p>A guide to the perilous world of reinforcement learning.</p>
</div>

    
  <div id="content_center">
    <div id="home_nav">
      <ul> 
         
            <li><a href=" /">Home</a></li>
         
            <li><a href=" /about/">About</a></li>
         
            <li><a href=" /posts/">All posts</a></li>
         
            <li><a href=" /notation/">Notaiton</a></li>
         
            <li><a href=" https://twitter.com/jmac_ai">Twitter</a></li>
        
      </ul>
    </div>
    <p>This site gathers my responses to reinforcement learning questions from various social media platforms, along with occasional AI-related opinion pieces.</p>
<p>Each answer begins with a concise explanation, accessible to those with relevant background knowledge. However, if you&rsquo;re here, you may not have <em>all</em> of the relevant background knowledge. Therefore, following each short answer, I provide the derivation of the answer from first principles. By the end of the derivation, you shoud find the short answer much more understandable.</p>
<p>You can learn more about
me and why I created this site in the <a href="/about/">about section</a>. You can find a comprehensive list of all posts in the <a href="/posts/">posts section</a>.</p>

    <h2>Recent posts</h2>
    
      <h3><a class="post_link" href="/posts/ddpg_grad/">Why is the DDPG gradient the product of the Q-function gradient and policy gradient?</a></h3>
      <p>The DDPG and DPG paper before it express the gradient of the objective $J(\pi)$ as the product of the policy and Q-function gradients:</p>
<p>$$
\nabla_\theta J(\pi) = E_{s \sim \rho^\pi} \left[\nabla_\theta \pi_\theta(s) \nabla_a Q(s, a) \rvert_{a \triangleq \pi_\theta(s)} \right].
$$</p>
<p>This expression looks a little scary, but all it&rsquo;s saying is to update your policy with gradient ascent to maximize the Q-function. This might not be obvious because the expression has been expanded with the multivariable chain rule of differentiation. If we undid the the chain rule, the product of &ldquo;gradients&rdquo; inside the expected value would be replaced with $\nabla_\theta Q(s, \pi_\theta(s))$.</p>
<p>Let&rsquo;s talk more about how we got here and why literature might express the result with the chain rule applied in the era of modern automatic differentiation.</p> <a href="/posts/ddpg_grad/">[See more]</a>
    
      <h3><a class="post_link" href="/posts/q_learning_doesnt_need_importance_sampling/">If Q-learning is off-policy, why doesn&#39;t it require importance sampling?</a></h3>
      <p>In off-policy learning, we evaluate the value function for a policy other than the one we are following in the environment. This difference creates a mismatch in state-action distributions. To account for this difference, some actor-critic methods use importance sampling. However, Q-learning, does not. There is a simple reason for that: In Q-learning, we only use samples to tell us about the effect of actions on the environment, not to estimate how good the policy action selection is. Let&rsquo;s make that more concrete with a simple example and re-derive the Q-learning and importance sampling approaches.</p> <a href="/posts/q_learning_doesnt_need_importance_sampling/">[See more]</a>
    
      <h3><a class="post_link" href="/posts/q_vs_v/">What is the difference between V(s) and Q(s,a)?</a></h3>
      <p>State value function $V(s)$ expresses how well the agent expects to do when it acts normally. $Q(s, a)$ is a counterfactual function that expresses how well the agent expects to do if first takes some potentially alternative action before acting normally.</p> <a href="/posts/q_vs_v/">[See more]</a>
    
  </div>

  <footer>
    <div style="text-align: center;">

</div>
  </footer>
</body>
</html>
