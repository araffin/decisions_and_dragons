<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
	<meta name="generator" content="Hugo 0.124.1"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Decisions &amp; Dragons</title>
<link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="stylesheet" href="/css/fonts.css">
    <link rel="stylesheet" href="/css/home.css">
    <link rel="stylesheet" href="/css/page_without_toc.css">
    <link rel="stylesheet" href="/css/page_with_toc.css">
    <link rel="stylesheet" href="/css/content_structure.css">
    <link rel="stylesheet" href="/css/mdx.css">


      <script src="/js/main.js"></script>


  
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']] , 
      tags: 'ams'
    }
  };
</script>
  
  <script>
window.addEventListener("hashchange", function () {
    window.scrollTo(window.scrollX, window.scrollY - 70);
});
</script>
</head>
<body>
    
<div id="splash">
<h1>Decisions &amp; Dragons</h1>
<p>A guide to the perilous world of reinforcement learning.</p>
</div>

    
  <div id="content_center">
    <div id="home_nav">
      <ul> 
         
            <li><a href=" /">Home</a></li>
         
            <li><a href=" /about/">About</a></li>
         
            <li><a href=" /posts/">All posts</a></li>
         
            <li><a href=" /notation/">Notaiton</a></li>
         
            <li><a href=" https://twitter.com/jmac_ai">Twitter</a></li>
        
      </ul>
    </div>
    <p>This site gathers my responses to reinforcement learning questions from various social media platforms, along with occasional AI-related opinion pieces.</p>
<p>Each answer begins with a concise explanation, accessible to those with relevant background knowledge. However, if you&rsquo;re here, you may not have <em>all</em> of the relevant background knowledge. Therefore, following each short answer, I derive the answer from first principles. By the end of the derivation, you shoud find the initial short answer much more understandable.</p>
<p>You can learn more about
me and why I created this site in the <a href="/about/">about section</a>. You can find a comprehensive list of all posts in the <a href="/posts/">posts section</a>.</p>

    <h2>Recent posts</h2>
    
      <h3><a class="post_link" href="/posts/horizon/">What is the &#34;horizon&#34; in reinforcement learning?</a></h3>
      <p>In reinforcement learning, an agent receives reward on each time step and the goal, loosely speaking, is to maximize the future reward received. But that doesn’t actually fully define the goal, because each decision can affect what the agent can do in the future. Consequently, we’re left with the question &ldquo;how does potential future reward affect our decision right now?&rdquo; The &ldquo;horizon&rdquo; in RL refers to how far into the future the agent cares about reward. You can have finite-horizon objectives, or even infinite-horizon objectives.</p> <a href="/posts/horizon/">[See more]</a>
    
      <h3><a class="post_link" href="/posts/q_learning_discrete_only/">Why doesn&#39;t Q-learning work with continuous actions?</a></h3>
      <p>Q-learning requires finding the action with the maximum Q-value in two places: (1) In the learning update itself; and (2) when extracting the policy from the learned Q-values. When there are a small number of discrete actions, you can simply enumerate the Q-values for each and pick the action with the highest value. However, this approach does not work with continuous actions, because there are an infinite number of actions to evaluate!</p> <a href="/posts/q_learning_discrete_only/">[See more]</a>
    
      <h3><a class="post_link" href="/posts/ddpg_grad/">Why is the DDPG gradient the product of the Q-function gradient and policy gradient?</a></h3>
      <p>The DDPG and DPG paper before it express the gradient of the objective $J(\pi)$ as the product of the policy and Q-function gradients:</p>
<p>$$
\nabla_\theta J(\pi) = E_{s \sim \rho^\pi} \left[\nabla_\theta \pi_\theta(s) \nabla_a Q(s, a) \rvert_{a \triangleq \pi_\theta(s)} \right].
$$</p>
<p>This expression looks a little scary, but it&rsquo;s conveying a straightforward concept: the gradient is the average of the Q-function&rsquo;s gradient with respect to the policy parameters, evaluated at the policy&rsquo;s selected action. That may not be obvious because the product of &ldquo;gradients&rdquo; (spoiler: there is some notation abuse) is the result of applying the multivariable chain rule of differentiation. If we were to reverse this step, the expected value would simplify to the more explicit expression $\nabla_\theta Q(s, \pi_\theta(s))$.</p>
<p>Let&rsquo;s rederive this result and explore why the literature presents it with an expansion of the chain rule instead of the more recognizable form.</p> <a href="/posts/ddpg_grad/">[See more]</a>
    
  </div>

  <footer>
    <div style="text-align: center;">

</div>
  </footer>
</body>
</html>
