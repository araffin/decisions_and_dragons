<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
	<meta name="generator" content="Hugo 0.124.1"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Decisions &amp; Dragons</title>
<link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="stylesheet" href="/css/fonts.css">
    <link rel="stylesheet" href="/css/home.css">
    <link rel="stylesheet" href="/css/page_without_toc.css">
    <link rel="stylesheet" href="/css/page_with_toc.css">
    <link rel="stylesheet" href="/css/content_structure.css">
    <link rel="stylesheet" href="/css/mdx.css">


      <script src="/js/main.js"></script>


  
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']] , 
      tags: 'ams'
    }
  };
</script>
  
  <script>
window.addEventListener("hashchange", function () {
    window.scrollTo(window.scrollX, window.scrollY - 70);
});
</script>
</head>
<body>
    
<div id="splash">
<h1>Decisions &amp; Dragons</h1>
<p>A guide to the perilous world of reinforcement learning.</p>
</div>

    
  <div id="content_center">
    <div id="home_nav">
      <ul> 
         
            <li><a href=" /">Home</a></li>
         
            <li><a href=" /about/">About</a></li>
         
            <li><a href=" /posts/">All posts</a></li>
         
            <li><a href=" /notation/">Notaiton</a></li>
         
            <li><a href=" https://twitter.com/jmac_ai">Twitter</a></li>
        
      </ul>
    </div>
    <p>This site gathers my responses to reinforcement learning questions from various social media platforms, along with occasional AI-related opinion pieces.</p>
<p>Each answer begins with a concise explanation, accessible to those with relevant background knowledge. However, if you&rsquo;re here, you may not have <em>all</em> of the relevant background knowledge. Therefore, following each short answer, I derive the answer from first principles. By the end of the derivation, you shoud find the initial short answer much more understandable.</p>
<p>You can learn more about
me and why I created this site in the <a href="/about/">about section</a>. You can find a comprehensive list of all posts in the <a href="/posts/">posts section</a>.</p>

    <h2>Recent posts</h2>
    
      <h3><a class="post_link" href="/posts/why_is_erb_useful/">Is experience replay only useful for neural net training?</a></h3>
       <a href="/posts/why_is_erb_useful/">[See more]</a>
    
      <h3><a class="post_link" href="/posts/off_policy_replay/">Why does experience replay require off-policy learning and how is it different from on-policy learning?</a></h3>
      <p>When you use an experience replay buffer, you save the most recent $k$ experiences of the agent, and sample data from that buffer for training. Typically, the agent does a step of training to update its policy for every step in the environment. Consequenlty, for any moment in time the vast majority of experiences in the buffer are generated with a different &ndash; earlier &ndash; policy than the current policy. And if the policy used to collect data is different than the policy being evaluated or improved, then you need an off-policy method.</p> <a href="/posts/off_policy_replay/">[See more]</a>
    
      <h3><a class="post_link" href="/posts/horizon/">What is the &#34;horizon&#34; in reinforcement learning?</a></h3>
      <p>In reinforcement learning, an agent receives reward on each time step and the goal, loosely speaking, is to maximize the future reward received. But that doesn’t actually fully define the goal, because each decision can affect what the agent can do in the future. Consequently, we’re left with the question &ldquo;how does potential future reward affect our decision right now?&rdquo; The &ldquo;horizon&rdquo; in RL refers to how far into the future the agent cares about reward. You can have finite-horizon objectives, or even infinite-horizon objectives.</p> <a href="/posts/horizon/">[See more]</a>
    
  </div>

  <footer>
    <div style="text-align: center;">

</div>
  </footer>
</body>
</html>
