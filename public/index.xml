<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Decisions &amp; Dragons</title>
    <link>http://localhost:1313/</link>
    <description>Recent content in Home on Decisions &amp; Dragons</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 30 Mar 2024 23:24:52 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>What is the difference between V(s) and Q(s,a)?</title>
      <link>http://localhost:1313/posts/q_vs_v/</link>
      <pubDate>Sat, 30 Mar 2024 23:24:52 -0400</pubDate>
      <guid>http://localhost:1313/posts/q_vs_v/</guid>
      <description>State value function $V(s)$ expresses how well the agent expects to do when it acts normally. $Q(s, a)$ is a counterfactual function that expresses how well the agent expects to do if first takes some potentially alternative action before acting normally.&#xA;A more precise definition Before making that more precise, let&amp;rsquo;s first add a point of clarity. Value functions (either V or Q) are always with respect to some policy $\pi$.</description>
    </item>
    <item>
      <title>Why does the policy gradient include a log probobability term?</title>
      <link>http://localhost:1313/posts/why_does_the_policy_gradient_include_log_prob/</link>
      <pubDate>Fri, 29 Mar 2024 20:07:54 -0400</pubDate>
      <guid>http://localhost:1313/posts/why_does_the_policy_gradient_include_log_prob/</guid>
      <description>Actually, it doesn&amp;rsquo;t! What you&amp;rsquo;re probably thinking of is the REINFORCE esitmate of the policy gradient that on-policy methods often use in practice. How we derive the REINFORCE estimate you&amp;rsquo;re familiar with and why we use it is something I found to be glossed over in literature, or presented in an overly technical way, but fortunately, it is not a hard concept to learn!&#xA;The short answer for where the log probability comes from is that REINFORCE uses a trick to make the policy gradient look like an expected value over the gradient, and the log probability manfiests as an importance sampling correction to make this work.</description>
    </item>
    <item>
      <title>What is the difference between model-based and model-free RL?</title>
      <link>http://localhost:1313/posts/model_free_vs_model_based/</link>
      <pubDate>Fri, 29 Mar 2024 19:50:17 -0400</pubDate>
      <guid>http://localhost:1313/posts/model_free_vs_model_based/</guid>
      <description>In reinforcement learning, the agent is not assumed to know how the environment will be affected by its actions. Model-based and model-free reinforcement learning tackle this problem in different ways. In model-based reinforcement learning, the agent learns a model of how the environment is affected by its actions and uses this model to determine good behavior. In model-free reinforcement learning, the agent learns how to act well without ever learning to predict precisely how the environment will be affectd by its actions.</description>
    </item>
    <item>
      <title>About</title>
      <link>http://localhost:1313/about/</link>
      <pubDate>Tue, 26 Mar 2024 20:17:59 -0400</pubDate>
      <guid>http://localhost:1313/about/</guid>
      <description>Reinforcement learning is an elegant and simple problem definition for autonomous decision-making agents that learn from their own experience. It subsumes questions of perception, prediction, planning, memory, etc &amp;ndash; all the relevant subfields of AI. But the methods to solve this simple problem are not so simple. The algorithm deriviations require understanding many important mathematical details and the methods in practice have many moving parts. The area is much more difficult to learn than supervised learning and other subfields of AI.</description>
    </item>
  </channel>
</rss>
