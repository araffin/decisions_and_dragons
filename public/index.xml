<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Decisions &amp; Dragons</title>
    <link>http://localhost:1313/</link>
    <description>Recent content in Home on Decisions &amp; Dragons</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 Mar 2024 20:07:54 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Why does the policy gradient include a log probobability term?</title>
      <link>http://localhost:1313/posts/why_does_the_policy_gradient_include_log_prob/</link>
      <pubDate>Fri, 29 Mar 2024 20:07:54 -0400</pubDate>
      <guid>http://localhost:1313/posts/why_does_the_policy_gradient_include_log_prob/</guid>
      <description>Actually, it doesn&amp;rsquo;t! What you&amp;rsquo;re probably thinking of is the REINFORCE esitmate of the policy gradient that on-policy methods often use in practice. How we derive the REINFORCE estimate you&amp;rsquo;re familiar with and why we use it is something I found to be glossed over in literature, or presented in an overly technical way, but fortunately, it is not a hard concept to learn!&#xA;The short answer for where the log probability comes from is that REINFORCE uses a trick to make the policy gradient look like an expected value over the gradient, and the log probability manfiests as an importance sampling correction to make this work.</description>
    </item>
    <item>
      <title>What is the difference between model-based and model-free RL?</title>
      <link>http://localhost:1313/posts/model_free_vs_model_based/</link>
      <pubDate>Fri, 29 Mar 2024 19:50:17 -0400</pubDate>
      <guid>http://localhost:1313/posts/model_free_vs_model_based/</guid>
      <description>In reinforcement learning, the agent is not assumed to know how the environment will be affected by its actions. Model-based and model-free reinforcement learning tackle this problem in different ways. In model-based reinforcement learning, the agent learns a model of how the environment is affected by its actions and uses this model to determine good behavior. In model-free reinforcement learning, the agent learns how to act well without ever learning to predict precisely how the environment will be affectd by its actions.</description>
    </item>
    <item>
      <title>About</title>
      <link>http://localhost:1313/about/</link>
      <pubDate>Tue, 26 Mar 2024 20:17:59 -0400</pubDate>
      <guid>http://localhost:1313/about/</guid>
      <description>Reinforcement learning is an elegant and simple problem definition for autonomous decision-making agents that learn from their own experience. It elegantly subsumes questions of perception, prediction, planning, memory, etc &amp;ndash; all the relevant subfields of AI. But the methods to solve this simple problem definition are not so simple, filled with important mathematical details and consistenting of many moving parts. The area is much more difficult to learn than supervised learning and other subfields of AI.</description>
    </item>
    <item>
      <title>Post 3</title>
      <link>http://localhost:1313/posts/post-3/</link>
      <pubDate>Wed, 15 Mar 2023 11:00:00 -0700</pubDate>
      <guid>http://localhost:1313/posts/post-3/</guid>
      <description>Occaecat aliqua consequat laborum ut ex aute aliqua culpa quis irure esse magna dolore quis. Proident fugiat labore eu laboris officia Lorem enim. Ipsum occaecat cillum ut tempor id sint aliqua incididunt nisi incididunt reprehenderit. Voluptate ad minim sint est aute aliquip esse occaecat tempor officia qui sunt. Aute ex ipsum id ut in est velit est laborum incididunt. Aliqua qui id do esse sunt eiusmod id deserunt eu nostrud aute sit ipsum.</description>
    </item>
    <item>
      <title>Post 2</title>
      <link>http://localhost:1313/posts/post-2/</link>
      <pubDate>Wed, 15 Feb 2023 10:00:00 -0700</pubDate>
      <guid>http://localhost:1313/posts/post-2/</guid>
      <description>Anim eiusmod irure incididunt sint cupidatat. Incididunt irure irure irure nisi ipsum do ut quis fugiat consectetur proident cupidatat incididunt cillum. Dolore voluptate occaecat qui mollit laborum ullamco et. Ipsum laboris officia anim laboris culpa eiusmod ex magna ex cupidatat anim ipsum aute. Mollit aliquip occaecat qui sunt velit ut cupidatat reprehenderit enim sunt laborum. Velit veniam in officia nulla adipisicing ut duis officia.&#xA;Exercitation voluptate irure in irure tempor mollit Lorem nostrud ad officia.</description>
    </item>
    <item>
      <title>Post 1</title>
      <link>http://localhost:1313/posts/post-1/</link>
      <pubDate>Sun, 15 Jan 2023 09:00:00 -0700</pubDate>
      <guid>http://localhost:1313/posts/post-1/</guid>
      <description>Tempor proident minim aliquip reprehenderit dolor et ad anim Lorem duis sint eiusmod. Labore ut ea duis dolor. Incididunt consectetur proident qui occaecat incididunt do nisi Lorem. Tempor do laborum elit laboris excepteur eiusmod do. Eiusmod nisi excepteur ut amet pariatur adipisicing Lorem.&#xA;Occaecat nulla excepteur dolore excepteur duis eiusmod ullamco officia anim in voluptate ea occaecat officia. Cillum sint esse velit ea officia minim fugiat. Elit ea esse id aliquip pariatur cupidatat id duis minim incididunt ea ea.</description>
    </item>
  </channel>
</rss>
