<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Decisions &amp; Dragons</title>
    <link>http://localhost:1313/</link>
    <description>Recent content in Home on Decisions &amp; Dragons</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Apr 2024 23:43:37 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>If Q-learning is off-policy, why doesn&#39;t it require importance sampling?</title>
      <link>http://localhost:1313/posts/q_learning_doesnt_need_importance_sampling/</link>
      <pubDate>Tue, 02 Apr 2024 23:43:37 -0400</pubDate>
      <guid>http://localhost:1313/posts/q_learning_doesnt_need_importance_sampling/</guid>
      <description>&lt;p&gt;In off-policy learning, we evaluate the value function for a policy other than the one we are following in the environment. This difference creates a mismatch in state-action distributions. To account for this difference, some actor-critic methods use importance sampling. However, Q-learning, does not. There is a simple reason for that: In Q-learning, we only use samples to tell us about the effect of actions on the environment, not to estimate how good the policy action selection is. Let&amp;rsquo;s make that more concrete with a simple example and re-derive the Q-learning and importance sampling approaches.&lt;/p&gt;</description>
    </item>
    <item>
      <title>What is the difference between V(s) and Q(s,a)?</title>
      <link>http://localhost:1313/posts/q_vs_v/</link>
      <pubDate>Sat, 30 Mar 2024 23:24:52 -0400</pubDate>
      <guid>http://localhost:1313/posts/q_vs_v/</guid>
      <description>&lt;p&gt;State value function $V(s)$ expresses how well the agent expects to do when it acts normally. $Q(s, a)$ is a counterfactual function that expresses how well the agent expects to do if first takes some potentially alternative action before acting normally.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why does the policy gradient include a log probobability term?</title>
      <link>http://localhost:1313/posts/why_does_the_policy_gradient_include_log_prob/</link>
      <pubDate>Fri, 29 Mar 2024 20:07:54 -0400</pubDate>
      <guid>http://localhost:1313/posts/why_does_the_policy_gradient_include_log_prob/</guid>
      <description>&lt;p&gt;Actually, it doesn&amp;rsquo;t! What you&amp;rsquo;re probably thinking of is the &lt;a href=&#34;https://people.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf&#34;&gt;REINFORCE&lt;/a&gt; &lt;em&gt;esitmate&lt;/em&gt; of the policy gradient that on-policy methods often use in practice. How we derive the REINFORCE estimate you&amp;rsquo;re familiar with and &lt;em&gt;why&lt;/em&gt; we use it is something I found to be glossed over in literature, or presented in an overly technical way, but fortunately, it is not a hard concept to learn!&lt;/p&gt;</description>
    </item>
    <item>
      <title>What is the difference between model-based and model-free RL?</title>
      <link>http://localhost:1313/posts/model_free_vs_model_based/</link>
      <pubDate>Fri, 29 Mar 2024 19:50:17 -0400</pubDate>
      <guid>http://localhost:1313/posts/model_free_vs_model_based/</guid>
      <description>&lt;p&gt;In reinforcement learning, the agent is not assumed to know how the environment will be affected by its actions. Model-based and model-free reinforcement learning tackle this problem in different ways. In model-based reinforcement learning, the agent learns a model of how the environment is affected by its actions and uses this model to determine good behavior. In model-free reinforcement learning, the agent learns how to act well without ever learning to predict precisely how the environment will be affectd by its actions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>About</title>
      <link>http://localhost:1313/about/</link>
      <pubDate>Tue, 26 Mar 2024 20:17:59 -0400</pubDate>
      <guid>http://localhost:1313/about/</guid>
      <description>Reinforcement learning is an elegant and simple problem definition for autonomous decision-making agents that learn from their own experience. It subsumes questions of perception, prediction, planning, memory, etc &amp;ndash; all the relevant subfields of AI. But the methods to solve this simple problem are not so simple. The algorithm deriviations require understanding many important mathematical details and the methods in practice have many moving parts. The area is much more difficult to learn than supervised learning and other subfields of AI.</description>
    </item>
    <item>
      <title>Math Notation Cheatsheet</title>
      <link>http://localhost:1313/notation/</link>
      <pubDate>Tue, 26 Mar 2024 20:17:59 -0400</pubDate>
      <guid>http://localhost:1313/notation/</guid>
      <description>In this section I outline the meaning of the mathematical notation I use. When appropriate (and possible), I also describe the meaning in simple python.&#xA;General math and statistics $f(x) \triangleq mx + b$ The $\triangleq$ indicates that the expresson on the left is defined to be the expression on the right, rather than an equivalence that is derived from mathemtical rules.&#xA;$(a, b)$ The set of real numbers between $a$ and $b$, excluding those values.</description>
    </item>
  </channel>
</rss>
