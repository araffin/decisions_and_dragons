<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Decisions &amp; Dragons</title>
    <link>http://localhost:1313/</link>
    <description>Recent content in Home on Decisions &amp; Dragons</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 20 Apr 2024 12:51:21 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Why is the DDPG gradient the product of the Q-function gradient and policy gradient?</title>
      <link>http://localhost:1313/posts/ddpg_grad/</link>
      <pubDate>Sat, 20 Apr 2024 12:51:21 -0400</pubDate>
      <guid>http://localhost:1313/posts/ddpg_grad/</guid>
      <description>&lt;p&gt;The DDPG and DPG paper before it express the gradient of the objective $J(\pi)$ as the product of the policy and Q-function gradients:&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;\nabla_\theta J(\pi) = E_{s \sim \rho^\pi} \left[\nabla_\theta \pi_\theta(s) \nabla_a Q(s, a) \rvert_{a \triangleq \pi_\theta(s)} \right].&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;This expression looks a little scary, but all it&amp;rsquo;s saying is to update your policy with gradient ascent to maximize the Q-function. This might not be obvious because the expression has been expanded with the multivariable chain rule of differentiation. If we undid the the chain rule, the product of &amp;ldquo;gradients&amp;rdquo; inside the expected value would be replaced with $\nabla_\theta Q(s, \pi_\theta(s))$.&lt;/p&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s talk more about how we got here and why literature might express the result with the chain rule applied in the era of modern automatic differentiation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>If Q-learning is off-policy, why doesn&#39;t it require importance sampling?</title>
      <link>http://localhost:1313/posts/q_learning_doesnt_need_importance_sampling/</link>
      <pubDate>Tue, 02 Apr 2024 23:43:37 -0400</pubDate>
      <guid>http://localhost:1313/posts/q_learning_doesnt_need_importance_sampling/</guid>
      <description>&lt;p&gt;In off-policy learning, we evaluate the value function for a policy other than the one we are following in the environment. This difference creates a mismatch in state-action distributions. To account for this difference, some actor-critic methods use importance sampling. However, Q-learning, does not. There is a simple reason for that: In Q-learning, we only use samples to tell us about the effect of actions on the environment, not to estimate how good the policy action selection is. Let&amp;rsquo;s make that more concrete with a simple example and re-derive the Q-learning and importance sampling approaches.&lt;/p&gt;</description>
    </item>
    <item>
      <title>What is the difference between V(s) and Q(s,a)?</title>
      <link>http://localhost:1313/posts/q_vs_v/</link>
      <pubDate>Sat, 30 Mar 2024 23:24:52 -0400</pubDate>
      <guid>http://localhost:1313/posts/q_vs_v/</guid>
      <description>&lt;p&gt;State value function $V(s)$ expresses how well the agent expects to do when it acts normally. $Q(s, a)$ is a counterfactual function that expresses how well the agent expects to do if first takes some potentially alternative action before acting normally.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why does the policy gradient include a log probobability term?</title>
      <link>http://localhost:1313/posts/why_does_the_policy_gradient_include_log_prob/</link>
      <pubDate>Fri, 29 Mar 2024 20:07:54 -0400</pubDate>
      <guid>http://localhost:1313/posts/why_does_the_policy_gradient_include_log_prob/</guid>
      <description>&lt;p&gt;Actually, it doesn&amp;rsquo;t! What you&amp;rsquo;re probably thinking of is the &lt;a href=&#34;https://people.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf&#34;&gt;REINFORCE&lt;/a&gt; &lt;em&gt;esitmate&lt;/em&gt; of the policy gradient that on-policy methods often use in practice. How we derive the REINFORCE estimate you&amp;rsquo;re familiar with and &lt;em&gt;why&lt;/em&gt; we use it is something I found to be glossed over in literature, or presented in an overly technical way, but fortunately, it is not a hard concept to learn!&lt;/p&gt;</description>
    </item>
    <item>
      <title>What is the difference between model-based and model-free RL?</title>
      <link>http://localhost:1313/posts/model_free_vs_model_based/</link>
      <pubDate>Fri, 29 Mar 2024 19:50:17 -0400</pubDate>
      <guid>http://localhost:1313/posts/model_free_vs_model_based/</guid>
      <description>&lt;p&gt;In reinforcement learning, the agent is not assumed to know how the environment will be affected by its actions. Model-based and model-free reinforcement learning tackle this problem in different ways. In model-based reinforcement learning, the agent learns a model of how the environment is affected by its actions and uses this model to determine good behavior. In model-free reinforcement learning, the agent learns how to act well without ever learning to predict precisely how the environment will be affectd by its actions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>About</title>
      <link>http://localhost:1313/about/</link>
      <pubDate>Tue, 26 Mar 2024 20:17:59 -0400</pubDate>
      <guid>http://localhost:1313/about/</guid>
      <description>Reinforcement learning (RL) is an elegant and simple problem definition for autonomous decision-making agents that learn from their own experience. It subsumes questions of perception, prediction, planning, memory, etc &amp;ndash; all the relevant subfields of AI. But the methods to solve this simple problem definition are not so simple. Unlike supervised learning, which typically involves training a single model, RL methods often involve training multiple interacting models. These interactions can lead to complex and unexpected mathematical challenges.</description>
    </item>
    <item>
      <title>Math Notation Cheatsheet</title>
      <link>http://localhost:1313/notation/</link>
      <pubDate>Tue, 26 Mar 2024 20:17:59 -0400</pubDate>
      <guid>http://localhost:1313/notation/</guid>
      <description>In this section I outline the meaning of the mathematical notation I use. When appropriate (and possible), I also describe the meaning in simple python.&#xA;General math and statistics $f(x) \triangleq mx + b$ The $\triangleq$ indicates that the expresson on the left is defined to be the expression on the right, rather than an equivalence that is derived from mathemtical rules.&#xA;$(a, b)$ The set of real numbers between $a$ and $b$, excluding those values.</description>
    </item>
  </channel>
</rss>
